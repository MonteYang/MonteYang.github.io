<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">

<meta name="baidu-site-verification" content="pzKra37kyC">
<meta name="google-site-verification" content="Tpp8m_k5m4Tzi6cI_Yzh5M10Yb0Vh4iUa8WMJqGE3kg">
<meta name="msvalidate.01" content="6848BAEF6B569BCE80C501D1FA6868F6">

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Monte Yang">


    <meta name="subtitle" content="朝着咸鱼的反方向努力">


    <meta name="description" content="❤MY">


    <meta name="keywords" content="3D Deep learning, Monte Yang, 深度学习, 算法">


<title>Reinforcement Learning-动态规划 | MonteYang&#39;s Blog</title>



    <link rel="icon" href="/image/logo32x32.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="MonteYang's Blog" type="application/atom+xml">
</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">MonteYang&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">MonteYang&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>

        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>

    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Reinforcement Learning-动态规划</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Monte Yang</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">2019-12-16&nbsp;&nbsp;10:10:42</a>
                        </span>
                    
                    
                        <span class="post-category">
                            Category:
                            
                                <a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><blockquote>
<p>马尔可夫决策过程的直接解法时间复杂度极高，因此强化学习方法的解法采用迭代式解法，迭代方法中<strong>最基础方法</strong>为动态规划方法。</p>
</blockquote>
<blockquote>
<p>之前提到解决序列决策问题有两种手段——学习与规划。<br>当有一个<strong>精确的环境模型</strong>时，才可以用动态规划去解。<br>Q learning等方法均由动态规划所推导而来。<br>动态规划方法利用了<strong>贝尔曼期望方程</strong>和<strong>贝尔曼最优方程</strong>。</p>
</blockquote>
<ul>
<li>动态：针对序列问题</li>
<li>规划：优化，得到策略</li>
</ul>
<p>能使用动态规划去解决的问题，需具有以下性质：</p>
<ul>
<li>最优子结构：满足最优性原理，优的解可以被分解成子问题的最优解。</li>
<li>交叠式子问题：子问题的解能够被多次利用</li>
</ul>
<p>恰好，MDP就满足这两个特性：</p>
<ul>
<li>回顾贝尔曼期望方程，满足递归形式。【当前状态的值函数 = E(当前的奖励+后继状态×衰减系数) 】，可以把问题分解成子问题</li>
<li>值函数的解可以重复利用</li>
</ul>
<p>使用动态规划解决强化学习问题时，要求知道 MDPs 的所有元素。这是因为强化学习问题中最重要的两个过程，<em>策略评价</em>和<em>策略优化</em>，需要满足：</p>
<ul>
<li>对于评价的过程：<ul>
<li>输入： MDP$⟨S; A; P; R; γ⟩$ 和策略 $π$</li>
<li>输出: 值函数 $v_π$</li>
</ul>
</li>
<li>对于优化的过程<ul>
<li>输入: MDP⟨S; A; P; R; γ⟩</li>
<li>输出：最优值函数 $\pi_*$</li>
</ul>
</li>
</ul>
<h1 id="策略评价"><a href="#策略评价" class="headerlink" title="策略评价"></a>策略评价</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>问题: 给定一个策略 π，求对应的值函数 $v_π(s)$(or $q_π(s; a))$。</p>
<blockquote>
<p><strong>对于动态规划问题</strong>，v函数和q函数是可以相互推导的。</p>
</blockquote>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol>
<li>直接解：<ul>
<li>可以直接求得精确解（上一篇文章中）</li>
<li>时间复杂度太高 $O(n^3)$</li>
</ul>
</li>
</ol>
<ol start="2">
<li>迭代解（√）： $v_1$ -&gt; $v_2$ -&gt; · · · -&gt; $v_π$<ul>
<li>利用贝尔曼期望方程迭代求解</li>
<li>可以收敛到 $v_π$</li>
</ul>
</li>
</ol>
<h2 id="利用贝尔曼方程进行迭代式策略评价"><a href="#利用贝尔曼方程进行迭代式策略评价" class="headerlink" title="利用贝尔曼方程进行迭代式策略评价"></a>利用贝尔曼方程进行迭代式策略评价</h2><p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E5%A4%87%E4%BB%BD%E5%9B%BE.png" alt></p>
<p>贝尔曼方程告诉我们，通过后继状态 $s’$ 的值函数，更新当前状态$s$ 的值函数</p>
<p><img src="../images/RL%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E7%AD%96%E7%95%A5%E8%AF%84%E4%BB%B7P1.png" alt></p>
<ul>
<li>$\pi$ 表示策略</li>
</ul>
<p>因此可以得到，</p>
<p><img src="../images/RL%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E7%AD%96%E7%95%A5%E8%AF%84%E4%BB%B7P2.png" alt></p>
<ul>
<li><strong>这里的 k 表示第 k 次迭代</strong></li>
</ul>
<p>从$v_1$一直迭代求到最后（能够收敛），可以得到最终的$v_\pi$</p>
<h2 id="同步备份下的迭代式策略评价算法"><a href="#同步备份下的迭代式策略评价算法" class="headerlink" title="同步备份下的迭代式策略评价算法"></a>同步备份下的迭代式策略评价算法</h2><p>关键词：</p>
<ul>
<li>备份（backup）：$v_{k+1}(s)$ 需要用到 $v_k(s’)$，用$v_k(s’)$更新当前状态。更新状态s的值函数，称为备份状态s。</li>
<li>同步（）：每一次更新，更新所有的状态</li>
<li>策略评价</li>
<li>迭代式</li>
</ul>
<p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E5%90%8C%E6%AD%A5%E5%A4%87%E4%BB%BD%E4%B8%8B%E7%9A%84%E8%BF%AD%E4%BB%A3%E5%BC%8F%E7%AD%96%E7%95%A5%E8%AF%84%E4%BB%B7%E7%AE%97%E6%B3%95.png" alt="同步备份下的迭代式策略评价算法"></p>
<!--TODO: 为加深理解，引入以下例子： -->


<hr>
<h1 id="策略提升"><a href="#策略提升" class="headerlink" title="策略提升"></a>策略提升</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>策略提升的目的是为了改进策略，强化学习的目的就是为了获得最优的策略。</p>
<p>给定一个策略$\pi$</p>
<ul>
<li>评价策略：求策略对应的值函数</li>
<li>改进策略（改进策略）：求得值函数 $v_{\pi}$ 后，根据贪婪的动作改进策略</li>
</ul>
<p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E8%B4%AA%E5%A9%AA.png" alt></p>
<p>在每个状态下，都选择能使q函数最大的动作。</p>
<ul>
<li>$v_{π′}(s) ≥ v_π(s);\forall{s}$，新的策略优于之前的策略</li>
<li>使得更新后的策略不差于之前的策略的过程称为策略提升</li>
<li>贪婪动作只是策略提升一种方式</li>
</ul>
<p>一般情况下，可能需要多次迭代（策略评价/策略提升）才能到达最优策略</p>
<h2 id="策略提升定理"><a href="#策略提升定理" class="headerlink" title="策略提升定理"></a>策略提升定理</h2><p>对于两个确定性策略 $π′$ 和 $π$，如果满足 $q_π(s, π′(s)) ≥ v_π(s)$，那么可以得到</p>
<p>$$v_{π’}(s) ≥ v_π(s)$$</p>
<blockquote>
<p>其中，$q_π(s, π′(s))$ 表示在当前状态s下，通过策略$π′$选择第一个动作，之后通过策略$\pi$进行动作选择，得到的期望回报值。</p>
</blockquote>
<hr>
<h1 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h1><h2 id="策略迭代的概念"><a href="#策略迭代的概念" class="headerlink" title="策略迭代的概念"></a>策略迭代的概念</h2><p>通过不断交替进行<em>策略评价</em>和<em>策略提升</em>，使策略收敛到最优的过程，称为<strong>策略迭代</strong>。</p>
<ul>
<li>策略评价: 求 $v_π$。使用方法：迭代式策略评价</li>
<li>策略提升: 提升策略 $π′ ≥ π$。使用方法: 贪婪策略提升</li>
</ul>
<p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3.png" alt></p>
<h2 id="收敛证明"><a href="#收敛证明" class="headerlink" title="收敛证明"></a>收敛证明</h2><p>策略提升停止时，当前策略$\pi’$达到最优策略$\pi$：</p>
<p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E7%AD%96%E7%95%A5%E6%8F%90%E5%8D%87%E6%94%B6%E6%95%9B.png" alt></p>
<p>此时，满足<strong>贝尔曼最优方程</strong>：</p>
<p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B.png" alt></p>
<h2 id="策略迭代算法"><a href="#策略迭代算法" class="headerlink" title="策略迭代算法"></a>策略迭代算法</h2><blockquote>
<p>策略迭代分为两部分：策略评价和策略提升。<br>当前的策略评价方法，选用的是迭代式策略评价方法，即通过不断进行迭代，计算出当前策略$\pi’$的v函数。</p>
</blockquote>
<p>利用迭代式策略评价的策略迭代算法为：</p>
<p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E8%BF%AD%E4%BB%A3%E5%BC%8F%E7%AD%96%E7%95%A5%E8%AF%84%E4%BB%B7%E7%9A%84%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95.png" alt></p>
<p>一般而言，策略评价需要一直进行迭代求解$v_\pi$。但是策略评价不一定要收敛到$v_\pi$，才能进行策略提升，可以引入提前停止的规则</p>
<ul>
<li>例如：值函数更新的 ∆ 足够小则停止</li>
<li>例如：限定迭代式策略评价只迭代 k 次。（当$k=1$时，是值迭代）</li>
</ul>
<h2 id="广义的策略迭代"><a href="#广义的策略迭代" class="headerlink" title="广义的策略迭代"></a>广义的策略迭代</h2><p>上述策略迭代方法中，指定了</p>
<ul>
<li>策略评价：选用<strong>迭代式</strong>方法</li>
<li>策略提升：选用<strong>贪婪动作</strong>方法</li>
</ul>
<p>几乎所有的强化学习算法都可以用广义策略迭代（Generalised Policy Iteration, GPI）来描述，GPI不限定两者的方法</p>
<ul>
<li>策略评价：任何策略评价方法</li>
<li>策略提升：任何策略提升方法（其他的还有：用一定的概率选择原策略，一定的概率选择贪婪策略）</li>
</ul>
<p>在策略评价中，值函数v只有符合当前策略的情况下才稳定（迭代后的值函数不变，如果值函数不符合当前策略，就一定能通过迭代进行改变）</p>
<p>策略提升中，策略只有在当前的值函数下，是贪婪的，才稳定。</p>
<p>因此，稳定状态下，分别收敛到最优的 $v_<em>(s), \pi_</em>(s)$。</p>
<h1 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h1><h2 id="强化学习的最优性原理"><a href="#强化学习的最优性原理" class="headerlink" title="强化学习的最优性原理"></a>强化学习的最优性原理</h2><p>任何最优的策略都能被分解成两部分</p>
<ul>
<li>最优的初始动作 $A_∗$</li>
<li>从后继状态 ${S}^′$ 开始沿着最优策略继续进行</li>
</ul>
<p>一个策略 $π(a|s)$ 能够实现从 $s$ 开始的最优值函数，$v_\pi(s)=v_*(s)$，当且仅当 对于任何从状态 $s$ 开始的后继状态 ${s}^{‘}$，$π$ 能实现从状态 $s^′$ 开始的最优值函数 $v_π(s^′) = v_∗(s^′)$</p>
<h2 id="值迭代-1"><a href="#值迭代-1" class="headerlink" title="值迭代"></a>值迭代</h2><p>根据最优性原理，只要知道 $v_∗(s^′)$（后继状态的最优值函数），即可以知道 $v_∗(s)$ （当前状态的最优值函数）<br>我们只需要选择一步动作即可，值迭代：</p>
<p><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E5%80%BC%E8%BF%AD%E4%BB%A3.png" alt></p>
<p>该式同时进行了策略评价和策略优化，max操作符表示进行了一次策略提升。</p>
<p>相当于从终止状态出发，递归地求解之前状态的值函数。</p>
<h2 id="值迭代和策略迭代的对比"><a href="#值迭代和策略迭代的对比" class="headerlink" title="值迭代和策略迭代的对比"></a>值迭代和策略迭代的对比</h2><h3 id="值迭代-2"><a href="#值迭代-2" class="headerlink" title="值迭代"></a>值迭代</h3><ul>
<li>$v_1$ -&gt; $v_2$ -&gt; $v_3$ -&gt; · · · -&gt; $v_∗$</li>
<li>没有显式的策略</li>
<li>迭代过程中的值函数可能不对应任何策略</li>
<li>效率较高</li>
<li>贝尔曼最优方程</li>
</ul>
<blockquote>
<p>贝尔曼最优方程<br><img src="../images/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B.png" alt></p>
</blockquote>
<h3 id="策略迭代-1"><a href="#策略迭代-1" class="headerlink" title="策略迭代"></a>策略迭代</h3><ul>
<li>$π_1$ -&gt; $v_1$ -&gt; $π_2$ -&gt; $v_2$ -&gt; · · · -&gt; $π_∗$ -&gt; $v_∗$</li>
<li>有显式的策略</li>
<li>迭代过程中的值函数对应了某个具体的策略</li>
<li>效率较低</li>
<li>贝尔曼期望方程 + 贪婪策略提升</li>
</ul>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Monte Yang</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="external nofollow noopener noreferrer">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Reinforcement-Learning/"># Reinforcement Learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">Back</a>
                <span>· </span>
                <a href="/">Home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Reinforcement-Learning-%E6%97%A0%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%951-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B.html">Reinforcement Learning-无模型方法1-蒙特卡洛</a>
            
            
            <a class="next" rel="next" href="/Reinforcement-Learning-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.html">Reinforcement Learning-马尔可夫决策过程</a>
            
        </section>


    </article>
</div>



    <div id="gitalk-container"></div>
    <script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>

<link rel="stylesheet" href="//unpkg.com/gitalk/dist/gitalk.css">
<script src="//unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: 'c3f96e8fb86bf653ad6c',
        clientSecret: '67954d29616c8cd146127aa2147f7e637aceb151',
        repo: 'MonteYang.github.io',
        owner: 'MonteYang',
        admin: 'MonteYang',
        id: md5(location.pathname),      
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 10,
        pagerDirection: 'last',
        createIssueManually: false,
        distractionFreeMode: false
      })
      gitalk.render('gitalk-container')
</script>



        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>2019 ~ 2020</span>
        <!-- <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span> -->
        <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a></span>
    </div>
</footer>

    </div>
</body>
</html>
