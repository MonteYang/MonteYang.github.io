<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">

<meta name="baidu-site-verification" content="pzKra37kyC">
<meta name="google-site-verification" content="Tpp8m_k5m4Tzi6cI_Yzh5M10Yb0Vh4iUa8WMJqGE3kg">
<meta name="msvalidate.01" content="6848BAEF6B569BCE80C501D1FA6868F6">

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Monte Yang">


    <meta name="subtitle" content="朝着咸鱼的反方向努力">


    <meta name="description" content="❤MY">


    <meta name="keywords" content="3D Deep learning, Monte Yang, 深度学习, 算法">


<title>Reinforcement Learning-无模型方法2-时间差分 | MonteYang&#39;s Blog</title>



    <link rel="icon" href="/image/logo32x32.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="MonteYang's Blog" type="application/atom+xml">
</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Monte&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Monte&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>

        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>

    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Reinforcement Learning-无模型方法2-时间差分</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Monte Yang</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">2019-12-16&nbsp;&nbsp;10:12:42</a>
                        </span>
                    
                    
                        <span class="post-category">
                            Category:
                            
                                <a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="时间差分方法简介"><a href="#时间差分方法简介" class="headerlink" title="时间差分方法简介"></a>时间差分方法简介</h1><ul>
<li>是强化学习中最为核心的想法，混合了DP和MC<ul>
<li>和 MC 类似， TD 也从历史经验中学习</li>
<li>和 DP 类似，使用后继状态的值函数更新当前状态的值函数</li>
</ul>
</li>
</ul>
<ul>
<li>属于无模型方法<ul>
<li>未知 P，R，需要智能体与环境进行交互，样本备份，需要充分的探索…</li>
</ul>
</li>
</ul>
<ul>
<li>同时利用了采样和贝尔曼方程</li>
</ul>
<ul>
<li>可以从不完整的片段中学习（通过<strong>自举法</strong>）<ul>
<li>可同时应用于片段性任务和连续性任务</li>
</ul>
</li>
</ul>
<ul>
<li><strong>通过估计来更新估计</strong></li>
</ul>
<blockquote>
<p><strong>自举法</strong><br>(bootstrapping) 又名拔靴法、自助法。通过对样本进行<strong>重采样</strong>得到的估计总体的方法。<br>重采样：对样本里的数据再采样一次。<br>强化学习的 MC 中，一条轨迹是一个样本。TD 中，把一条轨迹采样成小片段，再对片段进行采样，所以属于自举法。</p>
</blockquote>
<h1 id="时间差分评价"><a href="#时间差分评价" class="headerlink" title="时间差分评价"></a>时间差分评价</h1><p><strong>目的</strong>：给定策略 $π$，求其对应的值函数 $v_π$</p>
<blockquote>
<p>对比之前的蒙特卡洛算法。</p>
<ul>
<li>增量蒙特卡洛算法<ul>
<li>用实际回报值$G_t$去更新值函数 $V(S_t)$<br><img src="../images/RL-TD-MC%E6%9B%B4%E6%96%B0%E5%80%BC%E5%87%BD%E6%95%B0.png" alt></li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="时间差分评价算法"><a href="#时间差分评价算法" class="headerlink" title="时间差分评价算法"></a>时间差分评价算法</h2><p>时间差分算法 (Temporal-difference， TD)中，<strong>使用估计的回报值 $R_{t+1} + γV(S_{t+1})$ 去更新值函数 $V(S_t)$ (TD(0))</strong></p>
<p><img src="../images/RL-TD%E6%9B%B4%E6%96%B0%E5%80%BC%E5%87%BD%E6%95%B0.png" alt></p>
<p>其中：</p>
<ul>
<li>$R_{t+1} + \gamma V(S_{t+1})$ 称为 TD 目标</li>
<li>$\delta_t = R_{t+1} + \gamma V(S_{t+1}) − V(S_t)$ 称为 TD 误差</li>
<li>$\alpha$ 在0~1之间</li>
</ul>
<p>时间差分策略评价算法</p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-%E7%AE%97%E6%B3%95.png" alt></p>
<h2 id="评价算法对比：-TD-和-DP"><a href="#评价算法对比：-TD-和-DP" class="headerlink" title="评价算法对比： TD 和 DP"></a>评价算法对比： TD 和 DP</h2><p>DP中， 利用了贝尔曼方程去解强化学习问题</p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-DP.png" alt></p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-DP%E5%9B%BE.png" alt></p>
<p>而 TD 中，也利用了贝尔曼方程，但做了以下改动：</p>
<ul>
<li><p>全宽备份 $\rightarrow$ 样本备份： 并去掉了期望符号<br><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-%E5%AF%B9%E6%AF%94DP.png" alt></p>
</li>
<li><p>增加了学习率<br><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-%E5%AF%B9%E6%AF%94DP2.png" alt></p>
</li>
</ul>
<p>采样多的时候，收敛后，时间差分评价算法满足贝尔曼方程</p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-%E5%AF%B9%E6%AF%94DP3.png" alt></p>
<p>整体看来，在时间差分评价算法中，是利用TD目标和当前值函数的差来指导学习，所以称作时间差分</p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-TD%E5%9B%BE.png" alt></p>
<h2 id="评价算法对比：-TD-和-MC"><a href="#评价算法对比：-TD-和-MC" class="headerlink" title="评价算法对比： TD 和 MC"></a>评价算法对比： TD 和 MC</h2><p>在 MC 中，考虑的是整条轨迹。</p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-MC%E5%9B%BE.png" alt></p>
<ul>
<li>TD 在每一步之后都能在线学习； MC 必须等待回报值得到之后才能学习</li>
<li>TD 利用了马尔科夫性； MC 没有利用马尔科夫性</li>
<li>MC 有高方差，零偏差； TD 有低方差，和一些偏差</li>
</ul>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-%E5%A4%87%E4%BB%BD%E5%9B%BE.png" alt></p>
<h2 id="ML-和-RL-中的偏差-方差权衡"><a href="#ML-和-RL-中的偏差-方差权衡" class="headerlink" title="ML 和 RL 中的偏差/方差权衡"></a>ML 和 RL 中的偏差/方差权衡</h2><h3 id="ML-中的偏差和方差的trade-off"><a href="#ML-中的偏差和方差的trade-off" class="headerlink" title="ML 中的偏差和方差的trade-off"></a>ML 中的偏差和方差的trade-off</h3><p>在监督学习中，偏差/方差有另外的理解——<strong>欠拟合</strong>和<strong>过拟合</strong></p>
<ul>
<li>偏差大 (欠拟合): 预测值和样本之间的差</li>
<li>方差大 (过拟合): 样本值之间的方差, 学出的模型泛化能力差</li>
</ul>
<p>方差大意味着样本的置信度较差</p>
<p>机器学习方法都会在偏差和方差之间做 trade-off</p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE.png" alt></p>
<h3 id="RL-中的偏差和方差的trade-off"><a href="#RL-中的偏差和方差的trade-off" class="headerlink" title="RL 中的偏差和方差的trade-off"></a>RL 中的偏差和方差的trade-off</h3><ul>
<li>回报值 $G_t = R_{t+1} + \gamma R_{t+2} + … + {\gamma}^{T-t-1}R_T$ 是值函数 $v_\pi(S_t)$ 的无偏估计</li>
<li>真实的 TD 目标值 $R_{t+1} + γv_π(S_{t+1})$ 是值函数 $v_π(S_t)$ 的无偏估计</li>
<li>使用的 TD 目标值 $R_{t+1} + γV_π(S_{t+1})$ 是值函数 $v_π(S_t)$ 的有偏估计</li>
<li>TD 目标值的方差要远小于回报值<ul>
<li>回报值依赖于很多随机变量 $A_{t},S_{t+1},R_{t+1},A_{t+1},S_{t+2},R_{t+2},· · ·$</li>
<li>TD 目标值仅仅依赖于一个随机序列 $A_t,S_{t+1},R_{t+1}$</li>
</ul>
</li>
</ul>
<h1 id="时间差分优化"><a href="#时间差分优化" class="headerlink" title="时间差分优化"></a>时间差分优化</h1><ul>
<li>广义策略迭代<ul>
<li>策略评价: TD 策略评价， $Q = q_π$</li>
<li>策略提升: $\epsilon$-贪婪策略提升</li>
</ul>
</li>
<li>TD 优化相比 MC 优化有几点好处<ul>
<li>低方差</li>
<li>在线更新 (online)</li>
<li>不完整序列</li>
</ul>
</li>
</ul>
<h2 id="在策略-TD-优化——Sarsa"><a href="#在策略-TD-优化——Sarsa" class="headerlink" title="在策略 TD 优化——Sarsa"></a>在策略 TD 优化——Sarsa</h2><p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-SARSA.png" alt></p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-sarsa%E5%85%AC%E5%BC%8F.png" alt></p>
<p>值迭代</p>
<p>为什么是<strong>在策略</strong>？</p>
<p>执行的动作 A 是来自当前 Q 值下的 $\epsilon$-贪婪策略，构建 TD 目标值是的动作 A′ 是来自当前 Q 值下的 $\epsilon$-贪婪策略，这两者是同一个策略。</p>
<h2 id="离策略-TD-优化——Q学习"><a href="#离策略-TD-优化——Q学习" class="headerlink" title="离策略 TD 优化——Q学习"></a>离策略 TD 优化——Q学习</h2><p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-Q%E5%AD%A6%E4%B9%A0.png" alt></p>
<p><img src="../images/RL-%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86-Q%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B.png" alt></p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Monte Yang</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="external nofollow noopener noreferrer">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Reinforcement-Learning/"># Reinforcement Learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">Back</a>
                <span>· </span>
                <a href="/">Home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Reinforcement-Learning-%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC.html">Reinforcement Learning-值函数近似</a>
            
            
            <a class="next" rel="next" href="/Reinforcement-Learning-%E6%97%A0%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%951-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B.html">Reinforcement Learning-无模型方法1-蒙特卡洛</a>
            
        </section>


    </article>
</div>



    <div id="gitalk-container"></div>
    <script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>

<link rel="stylesheet" href="//unpkg.com/gitalk/dist/gitalk.css">
<script src="//unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: 'c3f96e8fb86bf653ad6c',
        clientSecret: '67954d29616c8cd146127aa2147f7e637aceb151',
        repo: 'MonteYang.github.io',
        owner: 'MonteYang',
        admin: 'MonteYang',
        id: md5(location.pathname),      
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 10,
        pagerDirection: 'last',
        createIssueManually: false,
        distractionFreeMode: false
      })
      gitalk.render('gitalk-container')
</script>



        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>2019 ~ 2020</span>
        <!-- <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span> -->
        <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a></span>
    </div>
</footer>

    </div>
</body>
</html>
