<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">

<meta name="baidu-site-verification" content="pzKra37kyC">
<meta name="google-site-verification" content="Tpp8m_k5m4Tzi6cI_Yzh5M10Yb0Vh4iUa8WMJqGE3kg">
<meta name="msvalidate.01" content="6848BAEF6B569BCE80C501D1FA6868F6">

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Monte Yang">


    <meta name="subtitle" content="朝着咸鱼的反方向努力">


    <meta name="description" content="❤MY">


    <meta name="keywords" content="3D Deep learning, Monte Yang, 深度学习, 算法">


<title>Reinforcement Learning-策略梯度算法 | MonteYang&#39;s Blog</title>



    <link rel="icon" href="/image/logo32x32.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="MonteYang's Blog" type="application/atom+xml">
</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Monte&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Monte&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>

        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>

    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Reinforcement Learning-策略梯度算法</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Monte Yang</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">2019-12-16&nbsp;&nbsp;10:14:42</a>
                        </span>
                    
                    
                        <span class="post-category">
                            Category:
                            
                                <a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="强化学习方法的分类"><a href="#强化学习方法的分类" class="headerlink" title="强化学习方法的分类"></a>强化学习方法的分类</h2><p>强化学习方法可以分为：基于值函数的方法、基于策略的方法和Actor-Critic。</p>
<ul>
<li>基于值函数的方法： 用<strong>值函数</strong>进行策略评价+策略优化，用值函数导出策略</li>
<li>基于策略的方法： 直接学习<strong>策略</strong></li>
<li>Actor-Critic： 学习<strong>值函数 + 策略</strong></li>
</ul>
<p>之前的强化学习方法都是基于值函数的方法，在值函数近似中，我们使用带参数w的函数去近似值函数，如下：</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC.png" alt></p>
<p>在基于值函数的方法中，策略是从值函数中导出的</p>
<ul>
<li>使用贪婪的方法导出<strong>最优策略</strong>（最终收敛得到的策略）</li>
<li>使用 $\epsilon$-贪婪方法导出<strong>行为策略</strong>（智能体真正做的策略）</li>
</ul>
<p>而基于策略的方法，直接将策略进行参数化。</p>
<h2 id="基于值函数的方法的局限性"><a href="#基于值函数的方法的局限性" class="headerlink" title="基于值函数的方法的局限性"></a>基于值函数的方法的局限性</h2><ul>
<li>针对确定性策略</li>
<li>存在策略退化： 收敛后，若存在误差，则很容易因为微小的值函数差，发生策略退化现象</li>
<li>难以处理高维度的状态/动作空间<ul>
<li>不能处理连续的状态/动作空间</li>
</ul>
</li>
<li>收敛速度慢： 反复进行策略迭代（策略评价+策略提升），速度较慢</li>
</ul>
<h2 id="策略梯度算法的优缺点"><a href="#策略梯度算法的优缺点" class="headerlink" title="策略梯度算法的优缺点"></a>策略梯度算法的优缺点</h2><p>策略梯度算法，也就是基于策略的方法，具有的优缺点：</p>
<p><strong>优点：</strong></p>
<ul>
<li>更好的收敛性</li>
<li>能够有效地处理高维和连续的动作空间</li>
<li>能够学到随机策略</li>
<li>不会导致策略退化</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>更容易收敛到局部最优值</li>
<li>难以评价一个策略，而且评价的<strong>方差较大</strong></li>
</ul>
<h2 id="策略模型的建模方式"><a href="#策略模型的建模方式" class="headerlink" title="策略模型的建模方式"></a>策略模型的建模方式</h2><p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95-%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F.png" alt></p>
<p>左侧2个是值函数方法，右侧2个是基于策略的方法。</p>
<h1 id="策略梯度算法"><a href="#策略梯度算法" class="headerlink" title="策略梯度算法"></a>策略梯度算法</h1><h2 id="策略梯度目标函数"><a href="#策略梯度目标函数" class="headerlink" title="策略梯度目标函数"></a>策略梯度目标函数</h2><p>在策略梯度算法中，我们用一个参数 θ 建模策略 $π_θ(s,a)$，如何寻找最优的参数 θ?</p>
<p>值函数近似时，优化的目标是使值函数的输出接近目标值。那么，如何不利用值函数，直接评价一个策略 $π_θ$ 的好坏？</p>
<h2 id="策略梯度的推导"><a href="#策略梯度的推导" class="headerlink" title="策略梯度的推导"></a>策略梯度的推导</h2><p>用 $τ$ 表示每次仿真的状态-行为序列 $S_0,A_0,… ,S_T,A_T$，每一个轨迹代表了强化学习的一个样本。轨迹的回报：</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E8%BD%A8%E8%BF%B9%E5%9B%9E%E6%8A%A5%E5%80%BC.png" alt></p>
<p>用 $P(τ; θ)$ 表示轨迹 $τ$ 出现的概率, 强化学习的<strong>目标函数</strong>可表示为</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0.png" alt></p>
<blockquote>
<p>对所有条轨迹进行累加: 轨迹的回报值 * 该轨迹出现的概率</p>
</blockquote>
<p>强化学习的目标是最大化整个过程的奖励，也就是最大化$U(\theta)$</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E7%9B%AE%E6%A0%87.png" alt></p>
<p>在上式中，不同的策略只是影响了不同轨迹出现的概率。</p>
<p>如何求解 $∇_θU(θ)$?</p>
<ul>
<li>$P(τ; θ)$ 未知</li>
<li>无法用一个可微分的数学模型直接表达 $U(θ)$</li>
</ul>
<p>策略梯度解决的问题是，即使未知 $U(θ)$ 的具体形式，也能求其梯度。</p>
<p>从似然率的角度：</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E4%BC%BC%E7%84%B6%E7%8E%87.png" alt></p>
<p>为什么要推导成这样的形式？</p>
<ul>
<li>$P(τ|θ)$ 可以通过 $π(a|s)$ 的模型表达 (后面会证明)</li>
<li>$R(τ)$ 可以通过采样的方式估计</li>
<li>期望符号 E 可以通过经验平均去估算</li>
</ul>
<p>利用当前策略 $π_θ$ <strong>采样 m 条轨迹，使用经验平均来估计梯度</strong>（MC形式）：</p>
<p>$$\bigtriangledown_\theta U(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} \bigtriangledown_\theta \log \mathbb{P}(\tau_i ; \theta) R(\tau_i)$$</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E7%90%86%E8%A7%A3.png" alt></p>
<ul>
<li>$∇_θ \log \mathbb{P} (τ; θ)$ 是轨迹 $τ$ 的概率随参数 $θ$ 变化最陡的方向<ul>
<li>沿正方向，轨迹出现的概率会变大</li>
<li>沿负方向，轨迹出现的概率会变小</li>
</ul>
</li>
<li>$R(τ)$ 控制了参数更新的方向和步长，正负决定了方向，大小决定了增大 (减小) 的幅度</li>
</ul>
<p>在策略梯度算法中，</p>
<ul>
<li>增大了高回报轨迹出现的概率，回报值越大增加越多</li>
<li>减少了低回报轨迹出现的概率，回报值越小减少越多</li>
</ul>
<p>注意到似然率梯度只是改变轨迹出现的概率，而没有尝试去改变轨迹</p>
<p>轨迹 $\tau$ 出现的概率 $\mathbb{P} (τ; θ)$ 是未知的，根据马尔科夫链：</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE.png" alt></p>
<p>由于状态转移概率 $P(s_{t+1}^{(i)} |s_t^{(i)}, a_t^{(i)})$ 中不包含参数 θ，因此求导的过程可以消掉，所以：</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E6%B1%82%E5%AF%BC.png" alt></p>
<p>因此，综上可得，我们可以在仅有可微分的策略模型 πθ 的情况下，求得 $∇_θU(θ)$</p>
<p>$$\hat{\eta} =∇<em>\theta U(\theta) \approx \frac{1}{m} \sum</em>{i=1}^{m} ∇_\theta \log \mathbb{P}(\tau_i ; \theta) R(\tau_i)$$</p>
<p>其中，</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E6%B1%82%E5%AF%BC2.png" alt></p>
<p>$\hat{\eta}$ 是 $∇_θU(θ)$ 的无偏估计</p>
<p>$$E[\hat{\eta}]=∇_θU(\theta)$$</p>
<h2 id="弥补策略梯度的缺陷"><a href="#弥补策略梯度的缺陷" class="headerlink" title="弥补策略梯度的缺陷"></a>弥补策略梯度的缺陷</h2><ul>
<li>方差大</li>
<li>如果所有的 R(τ) 都是正的，那么所有轨迹出现的概率都会增加</li>
</ul>
<p>我们可以通过以下方法减小方差：</p>
<ul>
<li>引入基线 (baseline)</li>
<li>修改回报函数</li>
<li>Actor-Critic 方法</li>
<li>优势函数</li>
</ul>
<h3 id="引入基线"><a href="#引入基线" class="headerlink" title="引入基线"></a>引入基线</h3><p>引入基线 b ，不影响策略梯度</p>
<p>$$∇<em>\theta U(\theta) \approx \frac{1}{m} \sum</em>{i=1}^{m} ∇<em>\theta \log \mathbb{P}(\tau_i ; \theta) R(\tau_i)=\frac{1}{m} \sum</em>{i=1}^{m} ∇_\theta \log \mathbb{P}(\tau_i ; \theta) (R(\tau_i)-b)$$</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E5%9F%BA%E7%BA%BF.png" alt></p>
<p>由上式可知，这是因为对基线求期望值，结果会抵消为0。</p>
<p>选择基线的方法：</p>
<ol>
<li>选择回报值函数的期望值</li>
</ol>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E5%9F%BA%E7%BA%BF1.png" alt></p>
<ol start="2">
<li>最小方差</li>
</ol>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E5%9F%BA%E7%BA%BF2.png" alt></p>
<h3 id="修改回报值函数"><a href="#修改回报值函数" class="headerlink" title="修改回报值函数"></a>修改回报值函数</h3><p>在当前的估计值值，对回报值 $R(\tau)$ 进行修改。</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E4%BF%AE%E6%94%B9%E5%9B%9E%E6%8A%A5%E5%80%BC%E5%87%BD%E6%95%B01.png" alt></p>
<p>由于将来的动作不依赖过去的奖励，因此我们可以修改回报值来降低方差。</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E4%BF%AE%E6%94%B9%E5%9B%9E%E6%8A%A5%E5%80%BC%E5%87%BD%E6%95%B02.png" alt></p>
<h1 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h1><h2 id="MC-策略梯度-REINFORCE"><a href="#MC-策略梯度-REINFORCE" class="headerlink" title="MC 策略梯度 (REINFORCE)"></a>MC 策略梯度 (REINFORCE)</h2><ul>
<li>使用梯度上升算法更新参数 θ</li>
<li>使用<strong>采样回报值</strong> $g_t$ 估计真实回报值</li>
</ul>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-REINFORCE.png" alt></p>
<blockquote>
<p>梯度上升：采用梯度上升是因为在策略梯度算法中，要最大化目标函数。<br><strong>采样回报值的方差很大</strong></p>
</blockquote>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-REINFORCE2.png" alt></p>
<h2 id="使用-Critic-函数减小方差"><a href="#使用-Critic-函数减小方差" class="headerlink" title="使用 Critic 函数减小方差"></a>使用 Critic 函数减小方差</h2><p>REINFORCE中的采样回报值$g_t$ 方差很大，<br>我们可以使用critic函数来估计回报值减小方差</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-Q%E5%87%BD%E6%95%B0.png" alt></p>
<blockquote>
<p>要注意的是：Actor-Critic 中的 Critic 只是为了减小策略梯度算法中估计<strong>回报值</strong>的方差。<strong>真正做决策的是 Actor</strong>（策略梯度算法）。</p>
</blockquote>
<p>Actor-Critic 算法维持两个参数：</p>
<ul>
<li>Critic 更新 Q 函数的参数 w</li>
<li>Actor 使用 Critic 的方向更新策略参数 θ</li>
</ul>
<p>将回报值替换为Q函数，近似策略梯度：</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E5%9B%9E%E6%8A%A5%E5%80%BCQ.png" alt></p>
<h2 id="使用优势函数减小误差"><a href="#使用优势函数减小误差" class="headerlink" title="使用优势函数减小误差"></a>使用优势函数减小误差</h2><p>优势函数：<br><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0.png" alt></p>
<p>表示在当前状态下，每个动作相对于平均情况，能创造多少优势。</p>
<p>即通过 V 函数估计基线，用 Q 函数估计回报函数。</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B02.png" alt></p>
<p>近似策略梯度</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B03.png" alt></p>
<h2 id="使用-TD-误差替代优势函数"><a href="#使用-TD-误差替代优势函数" class="headerlink" title="使用 TD 误差替代优势函数"></a>使用 TD 误差替代优势函数</h2><p>对于真实的值函数 $V^{π_θ}(s)， TD 误差为</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-TD%E8%AF%AF%E5%B7%AE.png" alt></p>
<p>TD 误差是优势函数的无偏估计</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-TD%E8%AF%AF%E5%B7%AE2.png" alt></p>
<p>使用 TD 误差来计算策略梯度</p>
<p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-TD%E8%AF%AF%E5%B7%AE3.png" alt></p>
<h1 id="策略梯度多种形式的总结"><a href="#策略梯度多种形式的总结" class="headerlink" title="策略梯度多种形式的总结"></a>策略梯度多种形式的总结</h1><p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-%E5%A4%9A%E7%A7%8D%E5%BD%A2%E5%BC%8F%E6%80%BB%E7%BB%93.png" alt></p>
<blockquote>
<p>其中 Advantage Actor-Critic 为 A2C 算法。<br>又由于TD误差是 优势函数 A的无偏估计，所以 TD Actor-Critic 也是 A2C，实际中的 A2C 也是用 TD Actor-Critic。</p>
</blockquote>
<p>该式中有两部分：</p>
<ul>
<li>求梯度的部分（∇）：控制着策略更新的<strong>方向</strong></li>
<li>$g_t$ 及其变种：控制着策略更新的<strong>步长</strong>（重要）</li>
</ul>
<p>Critic 使用了策略评价 (MC 或 TD) 来估计 $Q_π(s, a); A_π(s, a)$ 或<br>$V_π(s)$</p>
<h1 id="A2C-算法"><a href="#A2C-算法" class="headerlink" title="A2C 算法"></a>A2C 算法</h1><p><img src="../images/RL-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-A2C.png" alt></p>
<ol>
<li>通过在当前状态s下，执行动作a，获得奖励r、并到达下一个状态s’，</li>
<li>此时计算 TD 误差，</li>
<li>再根据 TD 误差 更新 Critic 和 Actor。</li>
</ol>
<blockquote>
<p>A2C 中为了满足样本独立同分布，常常使用多进程的思路。</p>
</blockquote>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Monte Yang</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="external nofollow noopener noreferrer">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Reinforcement-Learning/"># Reinforcement Learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">Back</a>
                <span>· </span>
                <a href="/">Home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Python-Matplotlib%E7%AE%80%E5%8D%95%E5%B0%8F%E7%BB%93.html">Python-Matplotlib简单小结</a>
            
            
            <a class="next" rel="next" href="/Reinforcement-Learning-%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC.html">Reinforcement Learning-值函数近似</a>
            
        </section>


    </article>
</div>



    <div id="gitalk-container"></div>
    <script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>

<link rel="stylesheet" href="//unpkg.com/gitalk/dist/gitalk.css">
<script src="//unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: 'c3f96e8fb86bf653ad6c',
        clientSecret: '67954d29616c8cd146127aa2147f7e637aceb151',
        repo: 'MonteYang.github.io',
        owner: 'MonteYang',
        admin: 'MonteYang',
        id: md5(location.pathname),      
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 10,
        pagerDirection: 'last',
        createIssueManually: false,
        distractionFreeMode: false
      })
      gitalk.render('gitalk-container')
</script>



        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>2019 ~ 2020</span>
        <!-- <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span> -->
        <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a></span>
    </div>
</footer>

    </div>
</body>
</html>
