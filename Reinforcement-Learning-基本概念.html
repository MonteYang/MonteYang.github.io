<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">

<meta name="baidu-site-verification" content="pzKra37kyC">
<meta name="google-site-verification" content="Tpp8m_k5m4Tzi6cI_Yzh5M10Yb0Vh4iUa8WMJqGE3kg">
<meta name="msvalidate.01" content="6848BAEF6B569BCE80C501D1FA6868F6">

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Monte Yang">


    <meta name="subtitle" content="朝着咸鱼的反方向努力">


    <meta name="description" content="❤MY">


    <meta name="keywords" content="3D Deep learning, Monte Yang, 深度学习, 算法">


<title>Reinforcement Learning-基本概念 | MonteYang&#39;s Blog</title>



    <link rel="icon" href="/image/logo32x32.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="MonteYang's Blog" type="application/atom+xml">
</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">MonteYang&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">MonteYang&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>

        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>

    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Reinforcement Learning-基本概念</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Monte Yang</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">2019-12-16&nbsp;&nbsp;10:08:42</a>
                        </span>
                    
                    
                        <span class="post-category">
                            Category:
                            
                                <a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>强化学习是智能体（Agent）与环境（Environment）不断交互, 不断提高自己的决策能力的过程.</p>
<ul>
<li>面向智能体的学习——通过与环境进行交互<ul>
<li>所学习的<strong>样本</strong>来自于<strong>与环境的交互</strong>。而非像传统机器学习的样本是现有的。</li>
</ul>
</li>
<li>通过<strong>试错和优化</strong>进行学习——用试错后的<strong>奖励（或惩罚）</strong>来学习</li>
</ul>
<blockquote>
<p><strong>总体过程</strong>：环境（Environment）会给智能体（Agent）一个观测值（Observation）（<em>全观测时,状态state和observation等价）</em>, 智能体接收到环境给的观测值之后会做出一个动作（Action）, 这个动作给予一个奖励（Reward）, 以及给出一个新的观测值. 智能体根据环境给予的奖励值去更新自己的策略（Policy）.</p>
</blockquote>
<p>强化学习的<strong>目的</strong> <em>就是为了得到最优的策略</em>。</p>
<hr>
<h1 id="2-强化学习的组成"><a href="#2-强化学习的组成" class="headerlink" title="2. 强化学习的组成"></a>2. 强化学习的组成</h1><h2 id="整体结构：环境和智能体"><a href="#整体结构：环境和智能体" class="headerlink" title="整体结构：环境和智能体"></a>整体结构：环境和智能体</h2><p><img src="/images/RL-%E6%95%B4%E4%BD%93%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="环境和智能体"></p>
<p>每个时刻 $t$:</p>
<ul>
<li>智能体（Agent）<ul>
<li>智能体执行动作$A_t$，并在环境中得到观测 $O_t$和奖励 $R_t$</li>
</ul>
</li>
<li>环境（Environment）<ul>
<li>环境会对智能体的动作 $A_t$的做出反应，然后发送新的观测 $O_t+1$ 和奖励 $R_t+1$</li>
</ul>
</li>
</ul>
<blockquote>
<p>智能体是我们能控制的部分，环境是我们无法控制的部分。<br>在不同的问题中，智能体和环境所指代的对象不同。</p>
</blockquote>
<hr>
<h2 id="2-1-奖励-Reward"><a href="#2-1-奖励-Reward" class="headerlink" title="2.1 奖励 Reward"></a>2.1 奖励 Reward</h2><p>奖励$R_t$是强化学习的<strong>核心</strong>，强化学习的目标就是<strong>最大化期望累积奖励</strong></p>
<ul>
<li><strong>特点</strong><ul>
<li>是<strong>标量</strong>，能够比较大小</li>
<li>表明这个智能体在时刻 $t$ 做得有多好</li>
<li>奖励不一定需要正负都有，全正或全负都可以，只要满足<strong>相对大小</strong>即可</li>
</ul>
</li>
</ul>
<blockquote>
<p>如果一个问题不满足奖励假设，就不能用强化学习解决！</p>
</blockquote>
<ul>
<li><p><strong>奖励值</strong>和<strong>回报值（Return）</strong>的比较</p>
<p>  回报值 $G_t$ 又称<strong>累积折扣奖励</strong>（cumulative discounted reward）。step $t$ 时的 return 为</p>
<p>  $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + …$$</p>
<p>  其中$\gamma$表示折扣因子，公式中重视每一步reward的程度随着时间衰减，是一个可调的超参数。</p>
<ul>
<li><p>$\gamma$ 越大，表示我们越关注长期奖励</p>
</li>
<li><p>$\gamma$ 越小，表示我们越关注短期奖励</p>
<p>智能体的任务就是去最大化累积奖励，即最大化整个过程的奖励.</p>
<p>因此，智能体的任务可以转化成<strong>最大化累积奖励的期望</strong>。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-2-状态-State"><a href="#2-2-状态-State" class="headerlink" title="2.2 状态 State"></a>2.2 状态 State</h2><h3 id="历史（history）"><a href="#历史（history）" class="headerlink" title="历史（history）"></a>历史（history）</h3><ul>
<li><strong>历史</strong> 是一个观测、动作和奖励的序列。是智能体在时间 $t$ 以前的所有的交互变量。</li>
</ul>
<p>$$H_t = (O_1, A_1, R_2, …, O_{T-1}, A_{T-1}, R_T, O_T)$$</p>
<p>根据历史：</p>
<ul>
<li>智能体选择动作 $A_t$</li>
<li>环境产生新的观测 $O_{t+1}$ 和奖励 $R_{t+1}$</li>
</ul>
<p>本质上，状态是历史的一个函数：<br>$$S_t = f(H_t)$$</p>
<blockquote>
<p>状态可以是<strong>全部历史</strong>或者<strong>部分历史</strong></p>
<ul>
<li>下棋时，棋盘当前的布局可以看做状态 $S_t = O_t$</li>
<li>打砖块游戏中，前几帧的观测可以看做状态 $S_{t} = O_{t-3}, O_{t-2}, O_{t-1}, O_t$</li>
</ul>
</blockquote>
<h3 id="环境状态-S-e-t"><a href="#环境状态-S-e-t" class="headerlink" title="环境状态 $S^e_t$"></a>环境状态 $S^e_t$</h3><p><img src="/images/RL-%E7%8E%AF%E5%A2%83%E7%8A%B6%E6%80%81.png" alt="环境状态"></p>
<ul>
<li>所有能够影响环境产生观测/奖励的数据都被认为是环境状态的一部分</li>
<li>环境状态一般是智能体观察不到的</li>
<li>即使环境状态 $S^e_t$ 可见的，一般也包含了不相关的信息</li>
</ul>
<h3 id="智能体状态-S-a-t"><a href="#智能体状态-S-a-t" class="headerlink" title="智能体状态 $S^a_t$"></a><strong>智能体状态</strong> $S^a_t$</h3><p><img src="/images/RL-%E6%99%BA%E8%83%BD%E4%BD%93%E7%8A%B6%E6%80%81.png" alt="智能体状态"></p>
<ul>
<li>所有能够影响智能体做出下一个动作的数据都被认为是智能体状态的一部分</li>
<li><strong>强化学习中使用的状态</strong></li>
<li>可能是历史的任何函数 $S_t = f(H_t)$</li>
</ul>
<blockquote>
<p>对于智能体来说，<strong>环境状态是未知的，智能体状态是已知的</strong><br>智能体通过智能体的状态来做出相应的动作<br>没有特殊说明的情况下，我们所说的状态均指智能体状态 $S_t = S^a_t$</p>
</blockquote>
<h3 id="全观测和部分观测"><a href="#全观测和部分观测" class="headerlink" title="全观测和部分观测"></a>全观测和部分观测</h3><ul>
<li>全观测：<ul>
<li>智能体能够<strong>观测到整个环境</strong>，即<strong>智能体状态等价于环境状态</strong></li>
<li>强化学习主要研究问题——马尔科夫决策过程</li>
</ul>
</li>
<li>部分观测：<ul>
<li>智能体<strong>不能完全观测</strong>到整个环境</li>
<li>建模为部分观测下的马尔科夫决策过程</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-3-动作-Action"><a href="#2-3-动作-Action" class="headerlink" title="2.3 动作 Action"></a>2.3 动作 Action</h2><ul>
<li>动作是智能体与环境交互的媒介</li>
<li>动作必须对环境起到<strong>一定的控制作用</strong></li>
<li>动作必须和奖励匹配</li>
</ul>
<hr>
<h1 id="3-智能体的组成"><a href="#3-智能体的组成" class="headerlink" title="3. 智能体的组成"></a>3. 智能体的组成</h1><p>智能体的组成主要包括：</p>
<ul>
<li>策略</li>
<li>值函数</li>
<li>模型</li>
</ul>
<h2 id="3-1-策略-Policy-pi"><a href="#3-1-策略-Policy-pi" class="headerlink" title="3.1 策略 Policy($\pi$)"></a>3.1 策略 Policy($\pi$)</h2><p>Policy是从状态到动作的映射, 用$\pi$表示, 它告诉智能体如何挑选下一步的action.</p>
<p>强化学习中有两类policy:</p>
<ol>
<li>Deterministic policy(确定性策略)</li>
</ol>
<p>在确定性策略中，输入一个状态，输出的是某一个<strong>确定的action</strong>.</p>
<ol>
<li>Stochastic policy(随机性策略)</li>
</ol>
<p>在随机性策略中，输入一个状态，输出的是<strong>每个action的概率分布</strong>.</p>
<hr>
<h2 id="3-2-值函数"><a href="#3-2-值函数" class="headerlink" title="3.2 值函数"></a>3.2 值函数</h2><ul>
<li>值函数是回报值的期望</li>
<li>值函数主要用于评价不同状态的好坏，指导动作的选择</li>
</ul>
<hr>
<h2 id="3-3-模型"><a href="#3-3-模型" class="headerlink" title="3.3 模型"></a>3.3 模型</h2><p>指智能体对环境的预测模型，分为两部分</p>
<ul>
<li>$\mathcal{P}$ 预测下一个状态是什么</li>
<li>$\mathcal{R}$ 预测下一个奖励是什么</li>
</ul>
<hr>
<h1 id="4-强化学习的分类"><a href="#4-强化学习的分类" class="headerlink" title="4. 强化学习的分类"></a>4. 强化学习的分类</h1><ul>
<li>按环境：<ul>
<li>全观测环境下的</li>
<li>部分观测环境下的</li>
</ul>
</li>
</ul>
<ul>
<li>按智能体成分：<ul>
<li>基于值函数：学习值函数</li>
<li>基于策略：学习策略</li>
<li>Actor Critic：同时学习值函数和策略</li>
</ul>
</li>
</ul>
<ul>
<li>按有无模型：<ul>
<li>无模型强化学习</li>
<li>基于模型的强化学习</li>
</ul>
</li>
</ul>
<ul>
<li>按使用的手段：<ul>
<li>传统强化学习</li>
<li>深度强化学习</li>
</ul>
</li>
</ul>
<hr>
<h1 id="5-学习（learning）和规划（planning）"><a href="#5-学习（learning）和规划（planning）" class="headerlink" title="5. 学习（learning）和规划（planning）"></a>5. 学习（learning）和规划（planning）</h1><p>解决<strong>序列决策问题</strong>有两个基本方法：</p>
<ul>
<li>强化学习<ul>
<li><strong>环境未知</strong></li>
<li>智能体与环境进行交互，不断改善策略</li>
</ul>
</li>
</ul>
<ul>
<li>规划<ul>
<li><strong>环境已知</strong></li>
<li>可以根据模型直接计算，不用交互</li>
</ul>
</li>
</ul>
<blockquote>
<p>环境模型不精确时，既利用环境进行规划，又与环境交互进行强化学习 –&gt; <strong>基于模型的强化学习</strong><br>当环境模型很精确时，可以直接用规划的方式求解</p>
</blockquote>
<hr>
<h1 id="6-探索（Exploration）和利用（Exploitation）"><a href="#6-探索（Exploration）和利用（Exploitation）" class="headerlink" title="6. 探索（Exploration）和利用（Exploitation）"></a>6. 探索（Exploration）和利用（Exploitation）</h1><ul>
<li>探索：<strong>发现环境中更多的信息</strong>；</li>
<li>利用：<strong>充分挖掘当前已知信息</strong>，来最大化回报值；</li>
</ul>
<p>两者trade-off，同等重要。</p>
<hr>
<h1 id="7-评价和优化"><a href="#7-评价和优化" class="headerlink" title="7. 评价和优化"></a>7. 评价和优化</h1><ul>
<li>评价：给定一个策略，评价该策略的好坏 –&gt; 求值函数</li>
<li>优化：改善策略 –&gt; 找到最优策略</li>
</ul>
<!-- ### 2.2. Episode
一个Episode由一系列的observation, reward, action组成.

$$(O_1, A_1, R_2, ..., O_{T-1}, A_{T-1}, R_T, O_T)$$

从 initial observation 到 terminal observation.  -->
<!--


### 2.5. 值函数Value Function

1. 状态值函数(V函数)
一个状态 state s 对应的状态值函数只关于状态:

$$V_\pi(s)=E_\pi(G_t|S_t=s)$$

对给定的s, V(s)是一个确定的值. 它表示, 从state s开始, 遵循策略$\pi$时的return的期望值.

2. 状态动作值函数(Q函数)
状态动作值函数关于状态和动作:

$$Q_\pi(s,a)=E_\pi(G_t|S_t=s,A_t=a)$$

它表示, 从state s开始, 遵循策略$\pi$采取动作a时的return的期望值.


## 3. 强化学习与其他机器学习

- 监督学习：**有即时标签**的学习（分类、回归问题）

- 非监督学习：**无标签**的学习（聚类问题）


- 强化学习：没有标签，只有奖励信号，**有延迟奖励**的学习问题。介于监督和非监督之间。
  - 强化学习的数据是**有时间相关性**的，**不满足独立同分布（iid）**。而传统机器学习的数据是iid的。因此传统机器学习的分布式系统对于强化学习不可用。
  - 智能体的动作可以影响之后的数据。 -->

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Monte Yang</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="external nofollow noopener noreferrer">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Reinforcement-Learning/"># Reinforcement Learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">Back</a>
                <span>· </span>
                <a href="/">Home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Reinforcement-Learning-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.html">Reinforcement Learning-马尔可夫决策过程</a>
            
            
            <a class="next" rel="next" href="/Python-axis%E7%9A%84%E7%90%86%E8%A7%A3.html">Python-axis的理解</a>
            
        </section>


    </article>
</div>



    <div id="gitalk-container"></div>
    <script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>

<link rel="stylesheet" href="//unpkg.com/gitalk/dist/gitalk.css">
<script src="//unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: 'c3f96e8fb86bf653ad6c',
        clientSecret: '67954d29616c8cd146127aa2147f7e637aceb151',
        repo: 'MonteYang.github.io',
        owner: 'MonteYang',
        admin: 'MonteYang',
        id: md5(location.pathname),      
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 10,
        pagerDirection: 'last',
        createIssueManually: false,
        distractionFreeMode: false
      })
      gitalk.render('gitalk-container')
</script>



        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>2019 ~ 2020</span>
        <!-- <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span> -->
        <span>© Monte Yang | Powered by <a href="https://hexo.io" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a></span>
    </div>
</footer>

    </div>
</body>
</html>
